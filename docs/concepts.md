# Core Concepts

This document explains the core abstractions in depth. For a quick overview, see the [README](../README.md).

---

## The Self-Play Paradigm

Traditional RL training follows a simple pattern: dataset → sample batch → generate rollouts → compute loss → update weights. Self-play doesn't fit this mold.

In self-play, the training data is generated by the model itself. The model might play both sides of a game, propose problems for itself to solve, or critique its own outputs. The "dataset" is dynamic—it evolves as the model improves.

This framework provides composable primitives that capture this paradigm:

1. **Actors**: The trainable entities (same model, different prompts)
2. **Episodes**: Self-contained interaction protocols
3. **Rubrics**: How outcomes are scored
4. **Credit Assigners**: How credit flows back to individual actions
5. **Arenas**: Where it all comes together

The key insight is that any self-play setup can be decomposed into these five abstractions.

---

## Actors

An **Actor** is a trainable entity. It's not a separate model—it's a configuration that defines how one "personality" of the model behaves.

```python
@dataclass
class Actor:
    id: str                          # Unique identifier
    system_prompt: str = ""          # Static instructions
    temperature: float = 1.0         # Sampling temperature
    max_tokens: Optional[int] = None # Token budget
```

### Why Actors Matter

Consider a proposer/solver setup:

```python
arena.add_actor(Actor(
    id="Proposer",
    system_prompt="You are a math teacher creating challenging problems...",
    temperature=0.9  # More creative
))

arena.add_actor(Actor(
    id="Solver",
    system_prompt="You are a student solving math problems step by step...",
    temperature=0.7  # More focused
))
```

Both actors share the same underlying policy (the model weights), but they receive different instructions and behave differently. When we train, both actors improve—the Proposer learns to generate harder problems, the Solver learns to solve them.

### Multi-Agent Games

In a two-player game like negotiation:

```python
arena.add_actor(Actor(id="Player0", system_prompt="You value Gold highly..."))
arena.add_actor(Actor(id="Player1", system_prompt="You value Wood highly..."))
```

Same model, opposite objectives. Credit assignment (see below) ensures each player gets relative advantages against their actor-specific peers.

---

## Episodes

An **Episode** is a self-contained rollout protocol. It defines:
- How actors interact
- What data flows between turns
- When the episode ends

```python
class Episode(ABC):
    @property
    @abstractmethod
    def episode_type(self) -> str:
        """Unique identifier for this episode type."""

    @property
    @abstractmethod
    def rubric(self) -> Rubric:
        """How to score this episode."""

    @abstractmethod
    async def rollout(
        self,
        arena: Arena,
        artifact: Any,
        state: Optional[EpisodeState] = None,
    ) -> EpisodeState:
        """Execute the episode logic."""
```

### Episode State

During execution, episodes track state in `EpisodeState`:

```python
@dataclass
class EpisodeState:
    trajectory: List[Step]           # Model calls so far
    current_actor: str = ""          # Who's turn it is
    data: Dict[str, Any]             # Custom episode data
    done: bool = False
    child_results: List[GenerateResult]  # From nested episodes
```

The `data` dict holds episode-specific state (game board, inventory, etc.). This is mutable during the episode but doesn't leak outside—see `get_extras()` below.

### Nested Episodes

Episodes can spawn sub-episodes. This is the key to Monte Carlo scoring and curriculum generation.

```python
async def rollout(self, arena, artifact, state):
    # Generate a proposal
    response = await self.call_model("Proposer", prompt, arena)
    question = parse(response.text)

    # Spawn 8 solver attempts
    requests = [
        EpisodeRequest("solve", question, is_trainable=False)
        for _ in range(8)
    ]
    results = await arena.generate_rollouts(requests)
    state.child_results.extend(results)

    return state
```

The `is_trainable=False` flag is crucial: solver completions inform the proposer's reward (via pass rate) but don't enter the training batch themselves. This prevents the proposer from gaming the reward by generating trivial problems.

### get_extras(): The Bridge to Rubrics

Episodes hide their internal state. Rubrics only see what episodes explicitly expose via `get_extras()`:

```python
def get_extras(self, state: EpisodeState) -> Dict[str, Any]:
    """Override to expose data to rubrics."""
    wins = sum(1 for r in state.child_results if r.rewards.get("Solver", 0) > 0)
    return {
        "pass_rate": wins / len(state.child_results),
        "question": state.data.get("question"),
    }
```

This clean separation means:
- Episodes can have arbitrary internal complexity
- Rubrics receive structured, well-defined data
- No hidden side effects or implicit dependencies

### Built-in Episode Types

The framework provides convenience base classes:

- **SingleTurnEpisode**: One prompt, one completion
- **MultiTurnEpisode**: Chat loop with `max_turns`
- **AlternatingActorsEpisode**: Two actors taking turns
- **ChatEpisode**: Full control over turn-taking logic

---

## Rubrics

A **Rubric** is a composable collection of reward functions.

```python
RewardFn = Callable[
    [Rollout, Arena],
    Union[Dict[str, float], Awaitable[Dict[str, float]]]
]

class Rubric:
    def __init__(
        self,
        funcs: List[RewardFn],
        weights: Optional[List[float]] = None,  # Default: all 1.0
    ): ...
```

### Reward Functions

Each reward function receives a `Rollout` and returns actor → reward mappings:

```python
def correctness_reward(rollout: Rollout, arena: Arena) -> Dict[str, float]:
    """Binary reward for correct answer."""
    answer = rollout.extras.get("parsed_answer")
    ground_truth = rollout.artifact.get("answer")
    correct = normalize(answer) == normalize(ground_truth)
    return {"Solver": 1.0 if correct else 0.0}

def brevity_reward(rollout: Rollout, arena: Arena) -> Dict[str, float]:
    """Bonus for concise answers."""
    tokens = len(rollout.steps[0].completion_text.split())
    bonus = max(0, 0.5 - tokens / 200)
    return {"Solver": bonus}
```

Combine them with weights:

```python
rubric = Rubric(
    funcs=[correctness_reward, brevity_reward],
    weights=[1.0, 0.5]  # correctness matters more
)
```

### LLM-as-Judge

Rubrics can call external models for judging:

```python
async def llm_judge(rollout: Rollout, arena: Arena) -> Dict[str, float]:
    prompt = f"Which response is better? A: {a} B: {b}"

    # Judge is NOT an actor—no training data produced
    response = await arena.call_model(prompt, temperature=0.3)

    winner = parse_winner(response.text)
    return {winner: 1.0, loser: -1.0}
```

The judge calls `arena.call_model()` without specifying an actor ID. This means:
- The completion doesn't enter the training batch
- The judge can be the same underlying model (self-consistency pressure)
- No reward hacking via judge manipulation

### Sync and Async

Rubrics handle both sync and async reward functions transparently:

```python
# Sync: simple string matching
def format_reward(rollout, arena):
    return {"Actor": 1.0 if "<answer>" in rollout.steps[0].completion_text else 0.0}

# Async: API call to external verifier
async def api_reward(rollout, arena):
    result = await verify_api(rollout.extras["code"])
    return {"Actor": result.score}

rubric = Rubric([format_reward, api_reward])  # Both work
```

---

## Credit Assignment

**Credit assignment** computes advantages from rewards. This is where REINFORCE becomes GRPO, PPO, or your custom algorithm.

```python
class CreditAssigner(ABC):
    @abstractmethod
    def compute(
        self,
        results: List[GenerateResult],
    ) -> Dict[Tuple[str, int], float]:
        """Return (rollout_id, step_index) → advantage."""
```

### Why Advantages ≠ Rewards

Raw rewards are absolute: "this rollout scored 0.7." Advantages are relative: "this rollout scored 0.2 better than average."

Relative advantages have lower variance, which is critical for stable RL training.

### GRPOCredit

Group Relative Policy Optimization computes advantages relative to the group mean:

```
advantage_i = reward_i - mean(rewards in group)
```

The key insight is **grouping**. GRPO groups by:

1. **Hierarchy level**: Top-level episodes form one group; each parent's children form independent groups
2. **Episode type**: Don't compare GSM8K rewards to negotiation rewards

```python
@dataclass
class GRPOCredit(CreditAssigner):
    normalize: bool = False      # Normalize to unit std
    positive_only: bool = False  # Clamp negative advantages to 0
```

### RAECredit (from SPIRAL)

Actor-conditioned Advantage Estimation maintains per-actor EMA baselines:

```
advantage = reward - baseline[actor_id]
baseline[actor_id] ← α * batch_mean + (1-α) * baseline[actor_id]
```

Why? In multi-agent games, actors can have structural advantages (e.g., first-mover advantage). EMA baselines adapt slowly to account for this.

```python
@dataclass
class RAECredit(CreditAssigner):
    gamma: float = 0.99  # EMA decay factor
```

### Per-Actor Computation

For multi-agent scenarios, advantages are computed per-actor:

```python
for actor_id in actor_ids:
    rewards = [r.rewards.get(actor_id, 0) for r in rollouts]
    actor_advantages[actor_id] = compute_grpo(rewards)

for step in all_steps:
    advantage[step] = actor_advantages[step.actor_id]
```

This ensures Player0 is compared to other Player0s, not Player1s.

---

## Arena

The **Arena** ties everything together. It's the stateful container that:
- Registers actors and episodes
- Manages artifact stores
- Schedules episode batches
- Orchestrates rollout generation
- Builds training batches

```python
class Arena:
    def __init__(
        self,
        client: InferenceClient,
        credit_assigner: Optional[CreditAssigner] = None,
    ):
        self.actors: Dict[str, Actor] = {}
        self.episodes: Dict[str, Episode] = {}
        self.stores: Dict[str, ArtifactStore] = {}
```

### Registration

```python
arena.add_actor(Actor(id="Proposer", system_prompt="..."))
arena.add_episode("propose", ProposerEpisode())
arena.add_store("questions")
```

### Batch Scheduling

Override `get_batch()` to define your training distribution:

```python
class MyArena(Arena):
    def get_batch(self) -> List[EpisodeRequest]:
        samples = self.stores["questions"].sample(k=self.batch_size)
        return [EpisodeRequest("solve", s.data) for s in samples]
```

This is where curriculum learning happens. You control what episodes run and with what artifacts.

### The Training Step

`arena.step()` orchestrates everything:

```python
async def step(self, concurrency=8) -> TrainingBatch:
    # 1. Get episode requests
    requests = self.get_batch()

    # 2. Tag with policy version
    version = await self.client.get_policy_version()
    for r in requests:
        r.meta["policy_version"] = version

    # 3. Generate rollouts in parallel
    results = await self.generate_rollouts(requests, concurrency)

    # 4. Credit assignment
    if self.credit_assigner:
        weights = self.credit_assigner.compute(results)
        apply_credit(results, weights)

    # 5. Build training batch
    return self.build_training_batch(results)
```

### Lifecycle Hooks

```python
async def on_train_start(self):
    """Called before training begins."""
    # Load data, initialize stores, warmup

async def on_train_end(self):
    """Called after training completes."""
    # Save artifacts, cleanup
```

---

## ArtifactStore

Simple in-memory storage with sampling:

```python
class ArtifactStore:
    def add(self, artifact: Artifact) -> None
    def sample(self, k: int = 1) -> List[Artifact]
    def sample_one(self) -> Optional[Artifact]
    def count(self) -> int
```

### Curriculum Learning

Stores enable self-generating curricula:

1. Proposer generates question → stored in `questions` store
2. Future batches sample from `questions` store
3. Hard questions get solved → inform proposer's reward
4. Proposer learns to generate questions at capability frontier

### Data Recycling

Stores can have `max_size` for FIFO eviction:

```python
arena.add_store("questions", ArtifactStore(max_size=1000))
```

This prevents unbounded memory growth while maintaining diversity.

---

## Design Patterns

### Pattern 1: get_extras() as Interface

Episodes hide implementation details. Rubrics receive structured data.

```python
# Inside episode
def get_extras(self, state):
    return {
        "winner": compute_winner(state.data),
        "score": state.data["final_score"],
    }

# Inside rubric
def reward_fn(rollout, arena):
    winner = rollout.extras["winner"]
    return {winner: rollout.extras["score"]}
```

### Pattern 2: Non-Trainable Sub-Episodes

Monte Carlo scoring without polluting training data:

```python
requests = [
    EpisodeRequest("solve", q, is_trainable=False)
    for _ in range(N)
]
```

### Pattern 3: Policy Versioning

Tag rollouts with policy version for staleness filtering:

```python
# In arena.step()
version = await self.client.get_policy_version()
request.meta["policy_version"] = version

# In trainer
if record.meta["policy_version"] < current_version - staleness_limit:
    skip(record)
```

### Pattern 4: LLM Judges Without Actors

Judges call `arena.call_model()` without actor_id:

```python
# This produces training data (actor completion)
response = await arena.call_model(prompt, actor_id="Solver")

# This doesn't (judge/oracle)
response = await arena.call_model(prompt)  # No actor_id
```

---

## Data Flow Summary

```
arena.step()
  ├─ get_batch() → List[EpisodeRequest]
  ├─ generate_rollouts()
  │   └─ episode.generate()
  │       ├─ rollout() → call_model(), spawn sub-episodes
  │       ├─ get_extras() → structured data
  │       └─ rubric.score() → step.reward, rollout.rewards
  ├─ credit_assigner.compute() → step.advantage
  └─ build_training_batch() → TrainingBatch
```

Every step in this pipeline is customizable. Override what you need, inherit the rest.
