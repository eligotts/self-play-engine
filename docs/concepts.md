# Core Concepts

This document goes deeper into the self-play abstractions. For a quick overview, see the [README](../README.md).

## The Problem with Standard RL Training

Traditional RL training follows a simple pattern: dataset → sample batch → generate rollouts → compute loss → update weights. Self-play doesn't fit this mold.

In self-play, the training data is generated by the model itself. The model might play both sides of a game, propose problems for itself to solve, or critique its own outputs. The "dataset" is dynamic - it evolves as the model improves.

So we need different primitives.

## Actors

An **Actor** is a trainable entity. It's not a separate model - it's a configuration that defines how one "personality" of the model behaves.

```python
@dataclass
class Actor:
    id: str                          # Unique identifier
    system_prompt: str = ""          # Static instructions
    max_tokens: Optional[int] = None # Token budget
```

Same underlying model weights, different behavior based on the system prompt and parameters. When we train, the single policy improves - but the actors maintain their distinct "personalities" through their prompts.

### Why Actors Matter

Consider a proposer/solver setup:

```python
arena.add_actor(Actor(
    id="Proposer",
    system_prompt="You are a math teacher creating challenging problems...",
))

arena.add_actor(Actor(
    id="Solver",
    system_prompt="You are a student solving math problems step by step...",
))
```

Both actors share the same weights. But the Proposer learns to generate harder problems while the Solver learns to solve them - the tension between them drives improvement.

In a two-player game:

```python
arena.add_actor(Actor(id="Player0", system_prompt="You value Gold highly..."))
arena.add_actor(Actor(id="Player1", system_prompt="You value Wood highly..."))
```

Same model, opposite objectives. Credit assignment (see below) ensures each player gets relative advantages against their actor-specific peers.

---

## Episodes

An **Episode** is a self-contained rollout protocol. It defines:
- How actors interact
- What data flows between turns
- When the episode ends
- How to score the outcome

```python
class Episode(ABC):
    @property
    @abstractmethod
    def episode_type(self) -> str:
        """Unique identifier for this episode type."""

    @property
    @abstractmethod
    def rubric(self) -> Rubric:
        """How to score this episode."""

    @abstractmethod
    async def rollout(
        self,
        arena: Arena,
        artifact: Any,
        state: Optional[EpisodeState] = None,
    ) -> EpisodeState:
        """Execute the episode logic."""
```

### EpisodeState

During execution, episodes track state:

```python
@dataclass
class EpisodeState:
    trajectory: List[Step]           # Model calls so far
    current_actor: str = ""          # Who's turn it is
    data: Dict[str, Any]             # Custom episode data
    done: bool = False
    child_results: List[GenerateResult]  # From nested episodes
```

The `data` dict is your scratch space - game board, inventory, pending offers, whatever. It's mutable during the episode but doesn't leak outside - that's what `get_extras()` is for.

### Calling the Model

Within an episode, you call the model through `self.call_model()`:

```python
response = await self.call_model(
    actor_id="Solver",
    prompt="What is 2 + 2?",
    arena=arena,
)
```

This returns a `ModelResponse` with the completion text, token IDs, and logprobs.

### Nested Episodes

Episodes can spawn sub-episodes. This is the key to Monte Carlo scoring and curriculum generation.

```python
async def rollout(self, arena, artifact, state):
    # Generate a proposal
    response = await self.call_model("Proposer", prompt, arena)
    question = parse(response.text)

    # Spawn 8 solver attempts
    requests = [
        EpisodeRequest("solve", question, is_trainable=False)
        for _ in range(8)
    ]
    results = await arena.generate_rollouts(requests)
    state.child_results.extend(results)

    return state
```

The `is_trainable=False` flag is crucial: solver completions inform the proposer's reward (via pass rate) but don't have to enter the training batch themselves. Up to the implementation to decide whether to train on the solver completions or not.

### get_extras(): The Bridge to Rubrics

Episodes hide their internal state. Rubrics only see what episodes explicitly expose via `get_extras()`:

```python
def get_extras(self, state: EpisodeState) -> Dict[str, Any]:
    """Override to expose data to rubrics."""
    wins = sum(1 for r in state.child_results if r.rewards.get("Solver", 0) > 0)
    return {
        "pass_rate": wins / len(state.child_results),
        "question": state.data.get("question"),
    }
```

This is how you pass information from your episode to your reward functions. The rubric receives `rollout.extras` which contains whatever you return from `get_extras()`.

Clean separation:
- Episodes can have arbitrary internal complexity
- Rubrics receive structured, well-defined data
- No hidden side effects or implicit dependencies

### Built-in Episode Types

The framework provides convenience base classes:

**SingleTurnEpisode** - One prompt, one completion. Override `get_prompt()` and done.

```python
class GSM8KEpisode(SingleTurnEpisode):
    def get_prompt(self, artifact: Dict) -> str:
        return artifact["question"]
```

**MultiTurnEpisode** - Chat loop with turn management. Override the callbacks:

```python
class NegotiationEpisode(MultiTurnEpisode):
    def get_initial_actor(self) -> str:
        return "Player0"

    def get_observation(self, state, arena, artifact) -> str:
        """Private info for current player - NOT in transcript."""
        return f"Your resources: {state.data['inventories'][state.current_actor]}"

    def env_response(self, state, arena, artifact) -> str:
        """Process action, return result - ADDED to transcript."""
        action = parse_action(state.last_completion_text)
        return execute_action(state.data, action)

    def is_done(self, state, arena, artifact) -> bool:
        return state.turn >= self.max_turns or state.data.get("game_over")
```

**Critical design detail**: `get_observation()` returns info that's private to the current player - it's NOT added to the transcript. This prevents leaking player-specific info to opponents (like personal resource values in negotiation).

---

## Rubrics

A **Rubric** is a composable collection of reward functions.

```python
RewardFn = Callable[
    [Rollout, Arena],
    Union[Dict[str, float], Awaitable[Dict[str, float]]]
]

class Rubric:
    def __init__(
        self,
        funcs: List[RewardFn],
        weights: Optional[List[float]] = None,  # Default: all 1.0
    ): ...
```

### Writing Reward Functions

Each reward function receives a `Rollout` and returns actor → reward mappings:

```python
def correctness_reward(rollout: Rollout, arena: Arena) -> Dict[str, float]:
    """Binary reward for correct answer."""
    answer = rollout.extras.get("parsed_answer")
    ground_truth = rollout.artifact.get("answer")
    correct = normalize(answer) == normalize(ground_truth)
    return {"Solver": 1.0 if correct else 0.0}

def brevity_reward(rollout: Rollout, arena: Arena) -> Dict[str, float]:
    """Bonus for concise answers."""
    tokens = len(rollout.steps[0].completion_text.split())
    bonus = max(0, 0.5 - tokens / 200)
    return {"Solver": bonus}
```

Combine them with weights:

```python
rubric = Rubric(
    funcs=[correctness_reward, brevity_reward],
    weights=[1.0, 0.5]  # Correctness matters more
)
```

## Credit Assignment

**Credit assignment** computes advantages from rewards. This is where REINFORCE becomes GRPO, RAE, or your custom algorithm.

```python
class CreditAssigner(ABC):
    @abstractmethod
    def compute(
        self,
        results: List[GenerateResult],
    ) -> Dict[Tuple[str, int], float]:
        """Return (rollout_id, step_index) → advantage."""
```

### GRPOCredit

Group Relative Policy Optimization computes advantages relative to the group mean:

```
advantage_i = reward_i - mean(rewards in group)
```

The key insight is **grouping**. GRPO groups by:

1. **Hierarchy level**: Top-level episodes form one group; each parent's children form independent groups
2. **Episode type**: Don't compare GSM8K rewards to negotiation rewards

```python
@dataclass
class GRPOCredit(CreditAssigner):
    normalize: bool = False      # Normalize to unit std
    positive_only: bool = False  # Clamp negative advantages to 0
```

With nested episodes:
```
Proposers: compared against other Proposers in the batch
  └── Solvers: compared against sibling Solvers (same parent)
```

### RAECredit (from SPIRAL)

Actor-conditioned Advantage Estimation maintains per-actor EMA baselines:

```
advantage = reward - baseline[actor_id]
baseline[actor_id] ← α * batch_mean + (1-α) * baseline[actor_id]
```

Why? In multi-agent games, actors can have structural advantages (e.g., first-mover advantage). EMA baselines adapt slowly to account for this.

```python
@dataclass
class RAECredit(CreditAssigner):
    decay: float = 0.95  # EMA decay factor
```

The baseline is updated AFTER advantage computation to keep the estimate unbiased.

### Per-Actor Computation

For multi-agent scenarios, advantages are computed per-actor:

```python
for actor_id in actor_ids:
    rewards = [r.rewards.get(actor_id, 0) for r in rollouts]
    actor_advantages[actor_id] = compute_grpo(rewards)

for step in all_steps:
    advantage[step] = actor_advantages[step.actor_id]
```

This ensures Player0 is compared to other Player0s, not Player1s.

## Arena

The **Arena** ties everything together. It's the stateful container that:
- Registers actors and episodes
- Manages artifact stores
- Schedules episode batches
- Orchestrates rollout generation
- Builds training batches

```python
class Arena:
    def __init__(
        self,
        client: InferenceClient,
        credit_assigner: Optional[CreditAssigner] = None,
    ):
        self.actors: Dict[str, Actor] = {}
        self.episodes: Dict[str, Episode] = {}
        self.stores: Dict[str, ArtifactStore] = {}
```

### Registration

```python
arena.add_actor(Actor(id="Proposer", system_prompt="..."))
arena.add_episode("propose", ProposerEpisode())
arena.add_store("questions")
```

### Batch Scheduling

Override `get_batch()` to define your training distribution:

```python
class MyArena(Arena):
    def get_batch(self) -> List[EpisodeRequest]:
        samples = self.stores["questions"].sample(k=self.batch_size)
        return [EpisodeRequest("solve", s.data) for s in samples]
```

This is where curriculum learning happens. You control what episodes run and with what artifacts.

### The Training Step

`arena.step()` orchestrates everything:

```python
async def step(self, concurrency=8) -> TrainingBatch:
    # 1. Get episode requests
    requests = self.get_batch()

    # 2. Tag with policy version
    version = await self.client.get_policy_version()
    for r in requests:
        r.meta["policy_version"] = version

    # 3. Generate rollouts in parallel
    results = await self.generate_rollouts(requests, concurrency)

    # 4. Credit assignment
    if self.credit_assigner:
        weights = self.credit_assigner.compute(results)
        apply_credit(results, weights)

    # 5. Build training batch
    return self.build_training_batch(results)
```

### Lifecycle Hooks

```python
async def on_train_start(self):
    """Called before training begins."""
    # Load data, initialize stores, warmup

async def on_train_end(self):
    """Called after training completes."""
    # Save artifacts, cleanup
```

## ArtifactStore

Simple in-memory storage with sampling:

```python
class ArtifactStore:
    def add(self, artifact: Artifact) -> None
    def sample(self, k: int = 1) -> List[Artifact]
    def sample_one(self) -> Optional[Artifact]
    def count(self) -> int
```

## Data Flow Summary

```
arena.step()
  ├─ get_batch() → List[EpisodeRequest]
  ├─ generate_rollouts()
  │   └─ episode.generate()
  │       ├─ rollout() → call_model(), spawn sub-episodes
  │       ├─ get_extras() → structured data for rubric
  │       └─ rubric.score() → step.reward, rollout.rewards
  ├─ credit_assigner.compute() → step.advantage
  └─ build_training_batch() → TrainingBatch
```

Every step in this pipeline is customizable. Override what you need, inherit the rest.
